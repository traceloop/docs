---
title: "Introduction"
---

Building reliable LLM applications means knowing whether a new prompt, model, or change of flow actually makes things better.

Experiments in Traceloop provide teams with a structured workflow for testing and comparing results across different prompt, model, and evaluator checks, all against real datasets.
## What You Can Do with Experiments

<CardGroup cols={2}>
 <Card title="Run Multiple Evaluators" icon="list-check">
    Execute multiple evaluation checks against your dataset
  </Card>
   <Card title="View Complete Results" icon="table">
    See all experiment run outputs in a comprehensive table view with relevant indicators and detailed reasoning
  </Card>
  <Card title="Compare Experiment Runs Results" icon="code-compare">
    Run the same experiment across different dataset versions to see how it affects your workflow
  </Card>
  <Card title="Custom Task Pipelines" icon="code">
    Add a tailored task to the experiment to create evaluator input. For example: LLM calls, semantic search, etc. 
  </Card>
</CardGroup>

## The Goal

Give you a repeatable, data-driven way to validate changes so you can ship improvements with confidence instead of relying on gut feel.

<Tip>
Experiments create immutable manifests that capture your test setup and results, making it easy to reproduce findings and track what actually worked.
</Tip>