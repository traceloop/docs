---
title: "Introduction"
---

Building reliable LLM applications means knowing whether a new prompt, model, or change of flow actually makes things better.

Experiments in Traceloop provide teams with a structured workflow for testing and comparing results across different prompt, model, and evaluator checks, all against real datasets.
## What You Can Do with Experiments

<CardGroup cols={2}>
  <Card title="Benchmark Against Real Data" icon="chart-line">
    Test new ideas against production-derived datasets to understand real-world performance
  </Card>
  <Card title="Compare Side by Side" icon="code-compare">
    See which prompt, model, or evaluator version truly performs better with clear comparisons
  </Card>
  <Card title="Track Progress Over Time" icon="chart-line-up">
    Monitor trends in quality, cost, and latency across different experiment runs
  </Card>
  <Card title="Share Results Easily" icon="share">
    Collaborate with your team without relying on spreadsheets or manual reports
  </Card>
</CardGroup>

## The Goal

Give you a repeatable, data-driven way to validate changes so you can ship improvements with confidence instead of relying on gut feel.

<Tip>
Experiments create immutable manifests that capture your test setup and results, making it easy to reproduce findings and track what actually worked.
</Tip>