---
title: "Python"
description: "Install OpenLLMetry for Python by following these 3 easy steps and get instant monitoring."
---

You can also check out our full working example of a RAG pipeline with Pinecone [here](https://github.com/traceloop/pinecone-demo).

<Steps>
<Step title="Install the SDK">

<Tip>
  Want our AI to do it for you? <a href="" target="_blank" id="vibekit-button" data-vibekit-token="k171j2wgqrg27p7zsr9kgv93kx7jtdmv"rel="noreferrer">Click here</a>
</Tip>

Run the following command in your terminal:

To install all optional dependencies, use the `all` tag:
<CodeGroup>

```bash pip
pip install "traceloop-sdk[all]"
```

```bash poetry
poetry add "traceloop-sdk[all]"
```

</CodeGroup>

Alternatively, you can install specific extras by listing them as a comma-separated list:
<CodeGroup>

```bash pip
pip install "traceloop-sdk[ollama,langchain]"
```

```bash poetry
poetry add "traceloop-sdk[ollama,langchain]"
```

</CodeGroup>

### Available Extras

- `mistralai` = `opentelemetry-instrumentation-mistralai`
- `openai` = `opentelemetry-instrumentation-openai`
- `ollama` = `opentelemetry-instrumentation-ollama`
- `anthropic` = `opentelemetry-instrumentation-anthropic`
- `cohere` = `opentelemetry-instrumentation-cohere`
- `crewai` = `opentelemetry-instrumentation-crewai`
- `google-generativeai` = `opentelemetry-instrumentation-google-generativeai`
- `pinecone` = `opentelemetry-instrumentation-pinecone`
- `qdrant` = `opentelemetry-instrumentation-qdrant`
- `langchain` = `opentelemetry-instrumentation-langchain`
- `lancedb` = `opentelemetry-instrumentation-lancedb`
- `chromadb` = `opentelemetry-instrumentation-chromadb`
- `transformers` = `opentelemetry-instrumentation-transformers`
- `together` = `opentelemetry-instrumentation-together`
- `llamaindex` = `opentelemetry-instrumentation-llamaindex`
- `milvus` = `opentelemetry-instrumentation-milvus`
- `haystack` = `opentelemetry-instrumentation-haystack`
- `bedrock` = `opentelemetry-instrumentation-bedrock`
- `sagemaker` = `opentelemetry-instrumentation-sagemaker`
- `replicate` = `opentelemetry-instrumentation-replicate`
- `vertexai` = `opentelemetry-instrumentation-vertexai`
- `watsonx` = `opentelemetry-instrumentation-watsonx`
- `weaviate` = `opentelemetry-instrumentation-weaviate`
- `alephalpha` = `opentelemetry-instrumentation-alephalpha`
- `marqo` = `opentelemetry-instrumentation-marqo`
- `groq` = `opentelemetry-instrumentation-groq`
- `mcp` = `opentelemetry-instrumentation-mcp`

In your LLM app, initialize the Traceloop tracer like this:

```python
from traceloop.sdk import Traceloop

Traceloop.init()
```

If you're running this locally, you may want to disable batch sending, so you can see the traces immediately:

```python
Traceloop.init(disable_batch=True)
```

</Step>
<Step title="Annotate your workflows">
<Frame>
  <img className="block dark:hidden" src="/img/workflow-light.png" />
  <img className="hidden dark:block" src="/img/workflow-dark.png" />
</Frame>
If you have complex workflows or chains, you can annotate them to get a better understanding of what's going on.
You'll see the complete trace of your workflow on Traceloop or any other dashboard you're using.

We have a set of [decorators](/openllmetry/tracing/annotations) to make this easier.
Assume you have a function that renders a prompt and calls an LLM, simply add `@workflow`.

<Warning>
  The `@aworkflow` decorator is deprecated and will be removed in a future
  version. Use `@workflow` for both synchronous and asynchronous operations.
</Warning>

<Tip>
  If you're using a [supported LLM framework](/openllmetry/tracing/supported#frameworks) - 
  we'll do that for you. No need to add any annotations to your code.
</Tip>

```python
from traceloop.sdk.decorators import workflow

@workflow(name="suggest_answers")
def suggest_answers(question: str):
  ...

# Works seamlessly with async functions too
@workflow(name="summarize")
async def summarize(long_text: str):
  ...
```

For more information, see the [dedicated section in the docs](/openllmetry/tracing/annotations).

</Step>
<Step title="Configure trace exporting">
Lastly, you'll need to configure where to export your traces.
The 2 environment variables controlling this are `TRACELOOP_API_KEY` and `TRACELOOP_BASE_URL`.

For Traceloop, read on. For other options, see [Exporting](/openllmetry/integrations/introduction).

### Using Traceloop Cloud

<Note>
  You need an API key to send traces to Traceloop.
  [Generate one in Settings](https://app.traceloop.com/settings/api-keys) by selecting
  a project and environment, then click **Generate API key**.

  ⚠️ **Important:** Copy the key immediately - it won't be shown again after you close or reload the page.

  [Detailed instructions →](/settings/managing-api-keys)
</Note>

Set the API key as an environment variable in your app named `TRACELOOP_API_KEY`:

```bash
export TRACELOOP_API_KEY=your_api_key_here
```

Done! You'll get instant visibility into everything that's happening with your LLM.
If you're calling a vector DB, or any other external service or database, you'll also see it in the Traceloop dashboard.

<Tip>
  **Not seeing traces?** Make sure you're viewing the correct project and environment in the
  dashboard that matches your API key. See [Troubleshooting](/settings/managing-api-keys#troubleshooting).
</Tip>

</Step>
</Steps>
