---
title: "Quick Start"
---

Playgrounds are interactive spreadsheet where you can orgenize your data and experiment with LLMs, evaluate outputs, and analyze data. 
Think of them as powerful workbenches for AI development that combine the flexibility of a spreadsheet with the power of LLM evaluation and execution.
Itâ€™s designed for everyone, from product managers and analysts to QA, data engineers, and software developers.

<Frame>
  <img
    className="block dark:hidden"
    src="/img/playground/play-list-light.png"
  />
  <img className="hidden dark:block" src="/img/playground/play-list-dark.png" />
</Frame>

## What are Playgrounds?

A Playground is a structured workspace consisting of:
- **Rows**: Individual data points or test cases
- **Columns**: Different types of data, computations, prompt executions or evaluations
- **Cells**: The intersection of rows and columns containing your actual data


## Create A Playground

Data can be added from different sources: 

1. CSV files
2. Json file
3. From A Dataset
4. From production spans

## Step 2: Add Input Data

Let's start with some customer feedback data:

1. Click **"Add Column"**
2. Select **"Custom Text"**
3. Name it "Customer Feedback"
4. Click **"Create Column"**

Add some sample data by clicking in the cells:
```
Row 1: "The product is great but delivery was slow"
Row 2: "Excellent customer service, very helpful staff"
Row 3: "The interface is confusing and hard to navigate"
```

## Step 3: Add Customer Information

1. Click **"Add Column"** again
2. Select **"Custom Text"**
3. Name it "Customer Name"
4. Add sample names:
```
Row 1: "Alice Johnson"
Row 2: "Bob Smith"  
Row 3: "Carol Davis"
```

## Step 4: Generate AI Responses

Now let's create AI-powered responses:

1. Click **"Add Column"**
2. Select **"Prompt"**
3. Name it "Response Generator"
4. In the prompt field, enter:
```
You are a helpful customer service representative. 

Customer: {{Customer Name}}
Feedback: {{Customer Feedback}}

Write a personalized, empathetic response that:
- Acknowledges their specific feedback
- Addresses any concerns mentioned
- Thanks them for their input
- Offers concrete next steps if needed

Response:
```

5. Configure the model:
   - **Model**: GPT-4 (recommended for quality)
   - **Temperature**: 0.7 (balance creativity and consistency)
   - **Max Tokens**: 200

6. Click **"Create Column"**

## Step 5: Evaluate Response Quality

Let's add automatic evaluation:

1. Click **"Add Column"**
2. Select **"Evaluation"**
3. Name it "Response Helpfulness"
4. Configure:
   - **Evaluator**: Helpfulness
   - **Target Column**: Response Generator
   - **Scale**: 1-10
5. Click **"Create Column"**

## Step 6: Execute Your Playground

Now let's run everything:

1. Click **"Execute All"** in the playground header
2. Watch the progress:
   - Input columns are already filled âœ…
   - Response Generator starts processing ðŸ”„
   - Helpfulness evaluation follows ðŸ”„

You'll see real-time status updates as each cell processes.

## Step 7: Review Results

Once execution completes, you should see:

**Response Generator Column**: Personalized AI responses for each customer
```
Example for Alice: "Hi Alice, thank you for your feedback about our product and delivery service. I'm glad to hear you're happy with the product quality! I sincerely apologize for the slow delivery experience..."
```

**Response Helpfulness Column**: Scores from 1-10 evaluating each response
```
Example scores: 8.5, 9.2, 7.8
```

## Step 8: Add More Evaluation

Let's add another evaluation dimension:

1. Click **"Add Column"**
2. Select **"Evaluation"** 
3. Name it "Response Relevancy"
4. Configure:
   - **Evaluator**: Relevancy
   - **Target Column**: Response Generator
   - **Context Column**: Customer Feedback
5. Click **"Create Column"**
6. Click **"Execute All"** to run the new evaluation

## Step 9: Analyze Results

Now you have a complete analysis pipeline! You can:

**View Individual Results**: Click on any cell to see detailed results
**Compare Across Rows**: See how responses vary by customer and feedback type
**Identify Patterns**: Look for feedback types that generate better/worse responses
**Export Data**: Use the Actions menu to export results as CSV or JSON

## Next Steps: Expand Your Playground

### Add More Sophisticated Logic
```
1. Add a "Sentiment Analysis" prompt column to categorize feedback
2. Create conditional responses based on sentiment
3. Add multiple response variations for A/B testing
```

### Enhance Evaluation
```
1. Add custom evaluators for your specific criteria
2. Include human annotation for subjective assessment
3. Set up multi-dimensional evaluation scorecards
```

### Scale Your Operation
```
1. Import larger datasets from CSV files
2. Connect to production data sources
3. Export results to datasets for reuse
```

## What You've Learned

In this quick start, you've:
- âœ… Created a playground with multiple column types
- âœ… Used variables to connect data between columns
- âœ… Executed AI prompts with real data
- âœ… Applied automated evaluation to assess quality
- âœ… Built a complete analysis workflow

## Common Next Steps

### For Content Teams
- **Blog Post Generation**: Create content variations and evaluate quality
- **Social Media**: Generate posts and evaluate engagement potential
- **Email Campaigns**: Personalize messages and test effectiveness

### For Customer Success
- **Support Response Training**: Generate and evaluate support responses
- **Feedback Analysis**: Analyze customer feedback at scale
- **Satisfaction Prediction**: Predict customer satisfaction from interactions

### For Product Teams  
- **Feature Feedback Analysis**: Categorize and analyze product feedback
- **User Experience Evaluation**: Assess UI/UX descriptions and recommendations
- **A/B Testing**: Compare different product descriptions or features

### For Research & Development
- **Model Comparison**: Test different AI models on the same tasks
- **Prompt Engineering**: Iterate on prompt designs systematically
- **Evaluation Development**: Create custom evaluation criteria for your domain

## Troubleshooting Quick Fixes

**Variables Not Working?**
- Check column name spelling (exact match required)
- Ensure referenced columns exist
- Use `{{Column Name}}` format with double curly braces

**Execution Stuck?**
- Check for failed dependencies (red X indicators)
- Verify API keys and model access
- Try executing columns individually to isolate issues

**Poor AI Results?**
- Add more context in your prompts
- Include specific examples of desired outputs
- Adjust temperature settings (lower for consistency, higher for creativity)

## Get Help

- ðŸ“š **[Full Documentation](./index)** - Comprehensive playground guides
- ðŸŽ¯ **[Core Concepts](./concepts/)** - Understand the fundamentals
- ðŸ”§ **[Column Types](./columns/)** - Learn about all 15 column types
- ðŸ“Š **[Evaluation System](./evaluation/)** - Master quality assessment
- ðŸ’¬ **Community Support** - Join our Discord for questions and tips

Ready to build more sophisticated playgrounds? Dive into the [complete documentation](./index) or explore specific [column types](./columns/) to unlock the full power of Traceloop Playgrounds!