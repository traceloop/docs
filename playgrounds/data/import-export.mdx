---
title: "Import & Export"
description: "Import data from files and export playground results in various formats"
---

# Import & Export

Playgrounds provide flexible import and export capabilities, allowing you to bring in data from various sources and export results for further analysis. This enables seamless integration with existing workflows and data pipelines.

## Import Options

### File Import
Import data from structured files with intelligent column mapping.

**Supported Formats**
- **JSON**: Nested data structures with automatic flattening
- **CSV**: Tabular data with header row detection
- **TSV**: Tab-separated values with custom delimiters

**Import Process**
1. **File Upload**: Drag and drop or browse for files
2. **Preview**: Review data structure and sample rows
3. **Column Mapping**: Map file fields to playground columns
4. **Data Validation**: Check for format issues and conflicts
5. **Import Execution**: Create rows and populate columns

### Dataset Import
Import data from existing Traceloop datasets with version control.

**Dataset Sources**
- **Dataset Versions**: Import specific versions of datasets
- **Dataset Subsets**: Import filtered portions of datasets
- **Cross-Project**: Import datasets from other projects (with permissions)

**Import Features**
- **Version Selection**: Choose specific dataset versions
- **Schema Mapping**: Automatic mapping of compatible fields
- **Incremental Import**: Add new data without duplicating existing rows
- **Metadata Preservation**: Maintain dataset lineage and versioning info

### Production Data Import
Import real-world data from traces and spans for analysis.

**Trace Import**
- **Span Selection**: Choose specific spans or entire traces
- **Time Range**: Import data from specific time periods
- **Filtering**: Apply filters by user, session, or other criteria
- **Metadata Extraction**: Extract relevant trace metadata

## Export Options

### File Export
Export playground data and results in various formats.

**Export Formats**
- **JSON**: Complete data with metadata and structure
- **CSV**: Tabular format for spreadsheet analysis
- **Excel**: Rich formatting with multiple sheets
- **Parquet**: Columnar format for data science workflows

**Export Configuration**
- **Column Selection**: Choose which columns to include
- **Row Filtering**: Export subsets based on criteria
- **Data Formatting**: Control number formats, date styles, etc.
- **Metadata Options**: Include execution metadata and timestamps

### Dataset Export
Export results back to Traceloop datasets for reuse.

**Export Features**
- **New Datasets**: Create new datasets from playground results
- **Dataset Updates**: Update existing datasets with new data
- **Version Control**: Create new dataset versions automatically
- **Schema Evolution**: Handle schema changes gracefully

## Import Workflows

### CSV Import Example
```
Step 1: Upload customer_feedback.csv
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_name   â”‚ feedback     â”‚ rating      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Alice Johnson   â”‚ Great serviceâ”‚ 5           â”‚
â”‚ Bob Smith       â”‚ Slow responseâ”‚ 2           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Column Mapping
customer_name â†’ Custom Text Column "Customer Name"
feedback      â†’ Custom Text Column "Feedback"
rating        â†’ Numeric Column "Rating"

Step 3: Import Results
âœ… 250 rows imported successfully
âš ï¸ 5 rows skipped due to validation errors
ğŸ“Š 3 columns created automatically
```

### JSON Import Example
```json
// Input JSON file
[
  {
    "user": {
      "name": "Alice Johnson",
      "tier": "Premium"
    },
    "conversation": {
      "messages": [
        {"role": "user", "content": "How do I reset my password?"},
        {"role": "assistant", "content": "You can reset your password by..."}
      ]
    },
    "metadata": {
      "timestamp": "2024-01-15T10:30:00Z",
      "session_id": "sess_123"
    }
  }
]

// Automatic Flattening
user.name         â†’ "Alice Johnson"
user.tier         â†’ "Premium"
conversation      â†’ Full conversation JSON
metadata.timestamp â†’ "2024-01-15T10:30:00Z"
```

### Dataset Integration
```
Playground: Customer Analysis
    â†“ (export results)
Dataset: "customer_insights_v2"
    â†“ (use in new playground)
Playground: Advanced Segmentation
    â†“ (export enhanced results)
Dataset: "customer_segments_v1"
```

## Column Mapping

### Automatic Mapping
The system attempts to automatically map imported data to appropriate column types:

**Text Data** â†’ Custom Text columns
**Numeric Data** â†’ Numeric columns
**Boolean Values** â†’ Boolean columns
**JSON Objects** â†’ JSON columns
**Date Strings** â†’ Custom Text (with date formatting)

### Manual Mapping
Fine-tune the mapping for optimal results:

**Column Type Selection**
- Override automatic type detection
- Choose specialized column types for specific data
- Configure column settings during import

**Data Transformation**
- Apply formatting rules during import
- Handle missing or null values
- Convert data types as needed

**Validation Rules**
- Set up validation for imported data
- Handle format inconsistencies
- Flag problematic rows for review

## Best Practices

### Import Preparation
**Data Quality**
- Clean data before import when possible
- Ensure consistent formatting across rows
- Handle missing values appropriately
- Validate data types and ranges

**File Organization**
- Use descriptive file names
- Include column headers in CSV files
- Organize JSON data consistently
- Document data sources and formats

**Import Planning**
- Review data structure before import
- Plan column types and organization
- Consider data volume and performance
- Prepare for validation and cleanup

### Export Optimization
**Data Selection**
- Export only necessary columns and rows
- Filter data to relevant subsets
- Consider downstream use cases
- Optimize file sizes for sharing

**Format Selection**
- Use CSV for spreadsheet analysis
- Use JSON for complex data structures
- Use Parquet for data science workflows
- Consider Excel for business presentations

**Documentation**
- Include metadata in exports
- Document column meanings and formats
- Provide data dictionaries when needed
- Track export versions and dates

## Advanced Features

### Batch Import
Import multiple files simultaneously:
- **Parallel Processing**: Handle multiple files concurrently
- **Progress Tracking**: Monitor import status across files
- **Error Handling**: Isolate failures to specific files
- **Bulk Operations**: Apply settings across multiple imports

### Incremental Import
Add new data without duplicating existing rows:
- **Duplicate Detection**: Identify existing rows automatically
- **Update Policies**: Choose how to handle duplicates
- **Change Tracking**: Monitor what data was added/updated
- **Rollback Options**: Undo imports if needed

### Custom Import Logic
Use code columns for complex import transformations:
```python
# Import transformation example
import json
import pandas as pd

def transform_import_data(raw_data):
    # Custom parsing logic
    df = pd.DataFrame(raw_data)
    
    # Extract nested fields
    df['user_name'] = df['user'].apply(lambda x: x.get('name'))
    df['user_tier'] = df['user'].apply(lambda x: x.get('tier'))
    
    # Format timestamps
    df['timestamp'] = pd.to_datetime(df['metadata.timestamp'])
    
    return df.to_dict('records')
```

### Export Automation
Automate exports with schedules and triggers:
- **Scheduled Exports**: Regular data exports to external systems
- **Trigger-Based**: Export when conditions are met
- **API Integration**: Direct export to external services
- **Webhook Notifications**: Notify systems when exports complete

## Integration Patterns

### ETL Workflows
```
External Data â†’ Import â†’ Transform (Code Columns) â†’ Export â†’ Data Warehouse
```

### Analysis Pipelines
```
Production Data â†’ Import â†’ Analysis (Prompts/Evaluations) â†’ Export â†’ Reports
```

### Feedback Loops
```
Export Results â†’ External Processing â†’ Import Enhanced Data â†’ Iterative Improvement
```

### Multi-Playground Workflows
```
Playground A â†’ Export â†’ Dataset â†’ Import â†’ Playground B â†’ Export â†’ Final Dataset
```

## Troubleshooting

### Import Issues

**File Format Problems**
```
Issue: "Unable to parse file format"
Solutions:
- Verify file format matches extension
- Check for encoding issues (use UTF-8)
- Validate JSON syntax
- Ensure CSV has proper delimiters
```

**Column Mapping Errors**
```
Issue: "Column type mismatch during import"
Solutions:
- Review automatic type detection
- Manually override column types
- Clean data format inconsistencies
- Use custom transformation logic
```

**Performance Issues**
```
Issue: "Import taking too long or timing out"
Solutions:
- Break large files into smaller chunks
- Remove unnecessary columns before import
- Use more efficient file formats (Parquet)
- Import during off-peak hours
```

### Export Issues

**File Size Limitations**
```
Issue: "Export file too large"
Solutions:
- Filter data to reduce export size
- Export in chunks or batches
- Use compressed formats
- Stream large exports instead of downloading
```

**Format Compatibility**
```
Issue: "Exported data not compatible with target system"
Solutions:
- Choose appropriate export format
- Configure data formatting options
- Use custom export transformations
- Validate export with target system
```

## Next Steps

Master data import and export by exploring:
- **[Dataset Integration](./datasets)** - Work with Traceloop datasets
- **[Trace Integration](./traces)** - Import production data
- **[Code Columns](../columns/computation/code)** - Custom data transformations
- **[API Integration](../columns/computation/api)** - Real-time data connections