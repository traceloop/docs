---
excerpt: Install OpenLLMetry for Node.js by following these 3 easy steps and get instant monitoring.
---

<Callout intent="warning">
If you’re on Next.js, follow the [Next.js guide](/docs/openllmetry/quick-start/next-js).
</Callout>

<Steps>
### Install the SDK
Run the following command in your terminal:

<CodeBlocks>
    <CodeBlock title="npm">
    ```bash
    npm install @traceloop/node-server-sdk
    ```
    </CodeBlock>
    <CodeBlock title="pnpm">
    ```bash
    pnpm add @traceloop/node-server-sdk
    ```
    </CodeBlock>
    <CodeBlock title="yarn">
    ```bash
    yarn add @traceloop/node-server-sdk
    ```
    </CodeBlock>
</CodeBlocks>

In your LLM app, initialize the Traceloop tracer like this:

```javascript
import * as traceloop from "@traceloop/node-server-sdk";

traceloop.initialize();
```

<Callout intent="warning">
Because of the way Javascript works, you must import the Traceloop SDK before importing any LLM module like OpenAI.
</Callout>

If you’re running this locally, you may want to disable batch sending, so you can see the traces immediately:

```javascript
traceloop.initialize({ disableBatch: true });
```

### Annotate your workflows

<Frame>
![openllmetry-node](https://fern-image-hosting.s3.amazonaws.com/traceloop/openllmetry-node.png)
</Frame>

If you have complex workflows or chains, you can annotate them to get a better understanding of what’s going on. You’ll see the complete trace of your workflow on Traceloop or any other dashboard you’re using.

We have a set of [methods and decorators](/docs/openllmetry/tracing/workflows-tasks-agents-and-tools) to make this easier. Assume you have a function that renders a prompt and calls an LLM, simply wrap it in a `withWorkflow()` function call.

We also have compatible Typescript decorators for class methods which are more convenient.

<Callout intent="launch">
If you’re using an LLM framework like Haystack, Langchain or LlamaIndex - we’ll do that for you. No need to add any annotations to your code.
</Callout>

<CodeBlocks>
    <CodeBlock title="Functions (async / sync)">
    ```javascript
    async function suggestAnswers(question: string) {
        return await withWorkflow({ name: "suggestAnswers" }, () => {
            // your code here...
        });
    }
    ```
    </CodeBlock>
    <CodeBlock title="Class Methods">
    ```javascript
    class MyLLM {
        @traceloop.workflow({ name: "suggest_answers" })
            async suggestAnswers(question: string) {
                // your code here...
        }
    }
    ```
    </CodeBlock>
</CodeBlocks>

For more information, see the [dedicated section in the docs](/openllmetry/tracing/workflows-tasks-agents-and-tools).

### Configure Trace Exporting

Lastly, you’ll need to configure where to export your traces. The 2 environment variables controlling this are `TRACELOOP_API_KEY` and `TRACELOOP_BASE_URL`.

For Traceloop, read on. For other options, see [Exporting](/openllmetry/integrations/overview).

## Using Traceloop Cloud

Go to [Traceloop](https://app.traceloop.com/), and create a new account. Then, click on **Environments** on the left-hand navigation bar. Or go to directly to https://app.traceloop.com/settings/api-keys. Click **Generate API Key** to generate an API key for the development environment and click **Copy API Key** to copy it over.

<Callout intent="warning">
Make sure to copy it as it won’t be shown again.
</Callout>

<Frame>
![openllmetry-node-2](https://fern-image-hosting.s3.amazonaws.com/traceloop/openllmetry-node-2.png)
</Frame>

Set the copied Traceloop’s API key as an environment variable in your app named `TRACELOOP_API_KEY`.

You're all set! You’ll get instant visibility into everything that’s happening with your LLM. If you’re calling a vector DB, or any other external service or database, you’ll also see it in the Traceloop dashboard.
</Steps>